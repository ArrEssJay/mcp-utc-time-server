# Fleet Management Documentation

This guide covers managing multiple Raspberry Pi deployments as a fleet.

## Overview

For organizations deploying multiple MCP Time Servers across different locations, fleet management ensures:

- Consistent configuration across devices
- Automated updates and rollbacks
- Centralized monitoring and alerting
- Secure key distribution
- Health dashboards

## Tools

### 1. Ansible (Recommended for < 100 devices)

```bash
# Install Ansible
pip3 install ansible

# Install Ansible community modules
ansible-galaxy collection install community.docker
```

### 2. Salt (For > 100 devices)

```bash
# Install Salt Master
curl -L https://bootstrap.saltstack.com -o install_salt.sh
sudo sh install_salt.sh -M

# Install Salt Minion (on each Pi)
curl -L https://bootstrap.saltstack.com -o install_salt.sh
sudo sh install_salt.sh
```

## Ansible Playbook

Create `ansible/inventory.yml`:

```yaml
all:
  children:
    stratum1:
      hosts:
        pi-gps-01:
          ansible_host: 192.168.1.101
          gps_enabled: yes
          pps_enabled: yes
          pps_gpio: 4
        pi-gps-02:
          ansible_host: 192.168.1.102
          gps_enabled: yes
          pps_enabled: yes
          pps_gpio: 4
    
    stratum2:
      hosts:
        pi-ntp-01:
          ansible_host: 192.168.1.201
          ntp_servers: "pi-gps-01,pi-gps-02"
        pi-ntp-02:
          ansible_host: 192.168.1.202
          ntp_servers: "pi-gps-01,pi-gps-02"
  
  vars:
    ansible_user: pi
    ansible_python_interpreter: /usr/bin/python3
```

Create `ansible/deploy.yml`:

```yaml
---
- name: Deploy MCP UTC Time Server Fleet
  hosts: all
  become: yes
  
  vars:
    repo_url: https://github.com/yourusername/mcp-utc-time-server.git
    install_path: /opt/mcp-utc-time-server
    api_key_vault: "{{ lookup('env', 'API_KEY_1') }}"
  
  tasks:
    - name: Update system packages
      apt:
        update_cache: yes
        upgrade: dist
    
    - name: Install required packages
      apt:
        name:
          - git
          - docker.io
          - docker-compose
          - python3-pip
        state: present
    
    - name: Clone repository
      git:
        repo: "{{ repo_url }}"
        dest: "{{ install_path }}"
        version: main
        force: yes
    
    - name: Create environment file
      template:
        src: templates/env.j2
        dest: "{{ install_path }}/.env"
        mode: '0600'
    
    - name: Pull Docker image
      community.docker.docker_image:
        name: ghcr.io/yourusername/mcp-utc-time-server
        tag: latest
        source: pull
    
    - name: Deploy container
      community.docker.docker_compose:
        project_src: "{{ install_path }}"
        files:
          - docker-compose.rpi.yml
        state: present
        restarted: yes
    
    - name: Wait for service to be healthy
      uri:
        url: "http://localhost:3000/health"
        status_code: 200
      register: result
      until: result.status == 200
      retries: 10
      delay: 3

- name: Configure Stratum 1 servers
  hosts: stratum1
  become: yes
  
  tasks:
    - name: Configure GPS serial
      lineinfile:
        path: /boot/config.txt
        line: "{{ item }}"
      loop:
        - "dtoverlay=disable-bt"
        - "enable_uart=1"
        - "dtoverlay=pps-gpio,gpiopin={{ pps_gpio }}"
      notify: reboot
    
    - name: Install GPS tools
      apt:
        name:
          - gpsd
          - gpsd-clients
          - pps-tools
        state: present
    
    - name: Configure gpsd
      template:
        src: templates/gpsd.j2
        dest: /etc/default/gpsd
      notify: restart gpsd

  handlers:
    - name: reboot
      reboot:
        reboot_timeout: 300
    
    - name: restart gpsd
      systemd:
        name: gpsd
        state: restarted
```

Create `ansible/templates/env.j2`:

```jinja2
# Generated by Ansible - Do not edit manually
API_KEY_1={{ api_key_vault }}
NTP_SERVERS={{ ntp_servers | default('time.cloudflare.com,time.google.com') }}
ENABLE_GPS={{ gps_enabled | default('no') }}
ENABLE_PPS={{ pps_enabled | default('no') }}
PPS_GPIO={{ pps_gpio | default('4') }}
LOCAL_STRATUM={{ stratum | default('10') }}
RUST_LOG=info
ENVIRONMENT={{ inventory_hostname }}
```

Run deployment:

```bash
# Deploy to all hosts
ansible-playbook -i ansible/inventory.yml ansible/deploy.yml

# Deploy to specific group
ansible-playbook -i ansible/inventory.yml ansible/deploy.yml --limit stratum1

# Check status
ansible all -i ansible/inventory.yml -m shell -a "docker ps"
```

## Centralized Monitoring

### Prometheus + Grafana

Create `monitoring/prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'mcp-time-servers'
    static_configs:
      - targets:
        - 'pi-gps-01:9090'
        - 'pi-gps-02:9090'
        - 'pi-ntp-01:9090'
        - 'pi-ntp-02:9090'
    
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
      - source_labels: [__address__]
        regex: '(.*):.*'
        target_label: device
```

Create `monitoring/docker-compose.yml`:

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    restart: unless-stopped
  
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    volumes:
      - grafana-data:/var/lib/grafana
      - ./dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=grafana-clock-panel
    restart: unless-stopped

volumes:
  prometheus-data:
  grafana-data:
```

## Health Checks

Create `scripts/fleet-health.sh`:

```bash
#!/bin/bash

HOSTS=(
  "pi-gps-01:192.168.1.101"
  "pi-gps-02:192.168.1.102"
  "pi-ntp-01:192.168.1.201"
  "pi-ntp-02:192.168.1.202"
)

echo "Fleet Health Check - $(date)"
echo "================================"

for host_entry in "${HOSTS[@]}"; do
  IFS=':' read -r name ip <<< "$host_entry"
  
  echo -n "Checking $name ($ip)... "
  
  # Check HTTP health
  if curl -sf "http://$ip:3000/health" > /dev/null 2>&1; then
    echo -n "✓ HTTP "
  else
    echo -n "✗ HTTP "
  fi
  
  # Check NTP sync
  ntp_status=$(ssh pi@$ip "docker exec mcp-utc-time ntpq -p -n" 2>/dev/null | grep -c '^\*')
  if [ "$ntp_status" -gt 0 ]; then
    echo -n "✓ NTP "
  else
    echo -n "✗ NTP "
  fi
  
  # Check disk space
  disk_usage=$(ssh pi@$ip "df -h / | tail -1 | awk '{print \$5}'" | tr -d '%')
  if [ "$disk_usage" -lt 80 ]; then
    echo -n "✓ Disk "
  else
    echo -n "⚠ Disk ($disk_usage%) "
  fi
  
  echo ""
done
```

## Automated Updates

Create `scripts/rolling-update.sh`:

```bash
#!/bin/bash

set -e

HOSTS=(
  "192.168.1.101"
  "192.168.1.102"
  "192.168.1.201"
  "192.168.1.202"
)

IMAGE="ghcr.io/yourusername/mcp-utc-time-server:latest"
WAIT_TIME=30

echo "Rolling update to $IMAGE"

for ip in "${HOSTS[@]}"; do
  echo "Updating $ip..."
  
  # Pull new image
  ssh pi@$ip "docker pull $IMAGE"
  
  # Restart container
  ssh pi@$ip "cd /opt/mcp-utc-time-server && docker-compose -f docker-compose.rpi.yml up -d"
  
  # Wait for health
  echo "Waiting for $ip to be healthy..."
  for i in {1..10}; do
    if curl -sf "http://$ip:3000/health" > /dev/null 2>&1; then
      echo "✓ $ip is healthy"
      break
    fi
    sleep 3
  done
  
  # Wait before next host
  echo "Waiting $WAIT_TIME seconds before next host..."
  sleep $WAIT_TIME
done

echo "Rolling update complete!"
```

## Configuration Management

Create `scripts/sync-configs.sh`:

```bash
#!/bin/bash

CONFIG_DIR="./fleet-configs"
HOSTS_FILE="./hosts.txt"

# Generate configs for each host
while IFS=',' read -r hostname ip stratum gps pps; do
  cat > "$CONFIG_DIR/$hostname.env" <<EOF
API_KEY_1=\${FLEET_API_KEY_1}
NTP_SERVERS=time.cloudflare.com,time.google.com
ENABLE_GPS=$gps
ENABLE_PPS=$pps
LOCAL_STRATUM=$stratum
RUST_LOG=info
ENVIRONMENT=$hostname
EOF
  
  # Deploy config
  scp "$CONFIG_DIR/$hostname.env" pi@$ip:/opt/mcp-utc-time-server/.env
  
  # Restart service
  ssh pi@$ip "cd /opt/mcp-utc-time-server && docker-compose -f docker-compose.rpi.yml restart"
done < "$HOSTS_FILE"
```

## Alerting

Create `monitoring/alertmanager.yml`:

```yaml
global:
  resolve_timeout: 5m

route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'slack-notifications'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
        channel: '#mcp-alerts'
        title: 'MCP Time Server Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

Create `monitoring/alert-rules.yml`:

```yaml
groups:
  - name: mcp_time_alerts
    interval: 30s
    rules:
      - alert: ServiceDown
        expr: up{job="mcp-time-servers"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          description: "{{ $labels.instance }} is down"
      
      - alert: NTPNotSynced
        expr: ntp_synced == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          description: "{{ $labels.instance }} NTP not synchronized"
      
      - alert: HighOffset
        expr: abs(ntp_offset_ms) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          description: "{{ $labels.instance }} has high time offset: {{ $value }}ms"
      
      - alert: DiskSpaceHigh
        expr: disk_usage_percent > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          description: "{{ $labels.instance }} disk usage {{ $value }}%"
```

## Backup and Disaster Recovery

Create `scripts/backup-fleet.sh`:

```bash
#!/bin/bash

BACKUP_DIR="./backups/$(date +%Y%m%d)"
mkdir -p "$BACKUP_DIR"

while read -r hostname ip; do
  echo "Backing up $hostname..."
  
  # Backup configuration
  ssh pi@$ip "tar czf - /opt/mcp-utc-time-server/.env /opt/mcp-utc-time-server/docker-compose.rpi.yml" \
    > "$BACKUP_DIR/$hostname-config.tar.gz"
  
  # Backup logs
  ssh pi@$ip "docker logs mcp-utc-time" > "$BACKUP_DIR/$hostname-logs.txt" 2>&1
  
done < hosts-list.txt

echo "Backups saved to $BACKUP_DIR"
```

## Best Practices

1. **Version Control**: Keep all configurations in git
2. **Secrets Management**: Use Ansible Vault or HashiCorp Vault
3. **Staged Rollouts**: Update canary hosts first
4. **Monitoring**: Set up 24/7 alerting
5. **Documentation**: Maintain runbooks for common issues
6. **Testing**: Test updates in staging environment first
7. **Backups**: Automated daily configuration backups
8. **Access Control**: Use SSH keys, disable password auth

## Troubleshooting Fleet Issues

```bash
# Check which hosts are unreachable
ansible all -i inventory.yml -m ping

# Gather facts from all hosts
ansible all -i inventory.yml -m setup

# Check Docker status on all hosts
ansible all -i inventory.yml -m shell -a "docker ps"

# View logs from specific host
ansible pi-gps-01 -i inventory.yml -m shell -a "docker logs mcp-utc-time --tail 100"

# Restart all services
ansible all -i inventory.yml -m shell -a "cd /opt/mcp-utc-time-server && docker-compose restart"
```
